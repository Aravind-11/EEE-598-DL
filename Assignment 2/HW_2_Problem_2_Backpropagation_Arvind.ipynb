{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Notebook: Backpropagation**\n",
        "\n",
        "This notebook runs the backpropagation algorithm on a deep neural network as described in the book (Section 7.4) **Understanding Deep Learning** by Simon Prince.\n",
        "\n",
        "Work through the cells below, running each cell in turn. In various places you will see the words \"TO DO\". Follow the instructions at these places to write code to complete the functions."
      ],
      "metadata": {
        "id": "L6chybAVFJW2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdIDglk1FFcG"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First let's define a neural network.  We'll just choose the weights and biases randomly for now"
      ],
      "metadata": {
        "id": "nnUoI0m6GyjC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set seed so we always get the same random numbers\n",
        "np.random.seed(44)\n",
        "\n",
        "# Number of layers\n",
        "K = 5\n",
        "# Number of neurons per layer\n",
        "D = 6\n",
        "# Input layer\n",
        "D_i = 1\n",
        "# Output layer\n",
        "D_o = 1\n",
        "\n",
        "# Make empty lists\n",
        "all_weights = [None] * (K+1)\n",
        "all_biases = [None] * (K+1)\n",
        "\n",
        "# Create input and output layers\n",
        "all_weights[0] = np.random.normal(size=(D, D_i))\n",
        "all_weights[-1] = np.random.normal(size=(D_o, D))\n",
        "all_biases[0] = np.random.normal(size =(D,1))\n",
        "all_biases[-1]= np.random.normal(size =(D_o,1))\n",
        "\n",
        "# Create intermediate layers\n",
        "for layer in range(1,K):\n",
        "  all_weights[layer] = np.random.normal(size=(D,D))\n",
        "  all_biases[layer] = np.random.normal(size=(D,1))"
      ],
      "metadata": {
        "id": "WVM4Tc_jGI0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Rectified Linear Unit (ReLU) function\n",
        "def ReLU(preactivation):\n",
        "  activation = preactivation.clip(0.0)\n",
        "  return activation"
      ],
      "metadata": {
        "id": "jZh-7bPXIDq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's run our random network.  The weight matrices $\\boldsymbol\\Omega_{1\\ldots K}$ are the entries of the list \"all_weights\" and the biases $\\boldsymbol\\beta_{1\\ldots K}$ are the entries of the list \"all_biases\"\n",
        "\n",
        "We know that we will need the preactivations $\\mathbf{f}_{0\\ldots K}$ and the activations $\\mathbf{h}_{1\\ldots K}$ for the forward pass of backpropagation, so we'll store and return these as well.\n"
      ],
      "metadata": {
        "id": "5irtyxnLJSGX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_network_output(net_input, all_weights, all_biases):\n",
        "\n",
        "  # Retrieve number of layers\n",
        "  K = len(all_weights) -1\n",
        "\n",
        "  # We'll store the pre-activations at each layer in a list \"all_f\"\n",
        "  # and the activations in a second list \"all_h\".\n",
        "  all_f = [None] * (K+1)\n",
        "  all_h = [None] * (K+1)\n",
        "\n",
        "  #We set all_h[0] to be the input, and all_f[K] will be the output\n",
        "  all_h[0] = net_input\n",
        "\n",
        "  # Run through the layers, calculating all_f[0...K-1] and all_h[1...K]\n",
        "  for layer in range(K):\n",
        "      # Update preactivations and activations at this layer\n",
        "      all_f[layer] = all_weights[layer] @ all_h[layer] + all_biases[layer]\n",
        "      all_h[layer+1] = ReLU(all_f[layer])\n",
        "\n",
        "  # Compute the output from the last hidden layer\n",
        "  all_f[K] = all_weights[K] @ all_h[K] + all_biases[K]\n",
        "\n",
        "  # Retrieve the output\n",
        "  net_output = all_f[K]\n",
        "\n",
        "  return net_output, all_f, all_h"
      ],
      "metadata": {
        "id": "LgquJUJvJPaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define input\n",
        "net_input = np.ones((D_i,1)) * 1.2\n",
        "# Compute network output\n",
        "net_output, all_f, all_h = compute_network_output(net_input,all_weights, all_biases)\n",
        "print(\"True output = %3.3f, Your answer = %3.3f\"%(-0.923, net_output[0,0]))"
      ],
      "metadata": {
        "id": "IN6w5m2ZOhnB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb124080-b7eb-470d-90fa-6b08b4b9607c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True output = -0.923, Your answer = -0.923\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's define a loss function.  We'll just use the least squares loss function. We'll also write a function to compute dloss_doutput"
      ],
      "metadata": {
        "id": "SxVTKp3IcoBF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def least_squares_loss(net_output, y):\n",
        "  return 0.5 * (net_output - y)**2\n",
        "\n",
        "def d_loss_d_output(net_output, y):\n",
        "    return net_output - y"
      ],
      "metadata": {
        "id": "6XqWSYWJdhQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = 30.0 #target value, what we want the network to output after convergence\n",
        "loss = least_squares_loss(net_output, y)\n",
        "print(\"y =\",y,\"Loss = \",loss)\n"
      ],
      "metadata": {
        "id": "njF2DUQmfttR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac7f7b6e-2c0b-4820-e2f4-4333c9a1bcdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y = 30.0 Loss =  [[478.11784299]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's compute the derivatives of the network.  We already computed the forward pass.  Let's compute the backward pass."
      ],
      "metadata": {
        "id": "98WmyqFYWA-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll need the indicator function\n",
        "def indicator_function(x):\n",
        "  x_in = np.array(x)\n",
        "  x_in[x_in>=0] = 1\n",
        "  x_in[x_in<0] = 0\n",
        "  return x_in\n",
        "\n",
        "# Main backward pass routine\n",
        "def backward_pass(all_weights, all_biases, all_f, all_h, y):\n",
        "  # We'll store the derivatives dl_dweights and dl_dbiases in lists as well\n",
        "  all_dl_dweights = [None] * (K+1)\n",
        "  all_dl_dbiases = [None] * (K+1)\n",
        "  # And we'll store the derivatives of the loss with respect to the activation and preactivations in lists\n",
        "  all_dl_df = [None] * (K+1)\n",
        "  all_dl_dh = [None] * (K+1)\n",
        "  # Again for convenience we'll stick with the convention that all_h[0] is the net input and all_f[k] in the net output\n",
        "\n",
        "  # Compute derivatives of the loss with respect to the network output\n",
        "  all_dl_df[K] = np.array(d_loss_d_output(all_f[K],y))\n",
        "\n",
        "  # Now work backwards through the network\n",
        "  for layer in range(K,-1,-1):\n",
        "    # TODO Calculate the derivatives of the loss with respect to the biases at layer from all_dl_df[layer].\n",
        "    # NOTE!  To take a copy of matrix X, use Z=np.array(X)\n",
        "    all_dl_dbiases[layer] = np.array(all_dl_df[layer])\n",
        "\n",
        "    # TODO Calculate the derivatives of the loss with respect to the weights at layer from all_dl_df[layer] and all_h[layer]\n",
        "    all_dl_dweights[layer] = np.array(all_dl_df[layer]) @ all_h[layer].T\n",
        "\n",
        "    # TODO: calculate the derivatives of the loss with respect to the activations from weight and derivatives of next preactivations\n",
        "    all_dl_dh[layer] = all_weights[layer].T @ all_dl_df[layer]\n",
        "\n",
        "    if layer > 0:\n",
        "      # TODO Calculate the derivatives of the loss with respect to the pre-activation f\n",
        "      all_dl_df[layer-1] = indicator_function(all_f[layer-1]) * all_weights[layer].T @ all_dl_df[layer]\n",
        "\n",
        "  return all_dl_dweights, all_dl_dbiases"
      ],
      "metadata": {
        "id": "LJng7WpRPLMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_dl_dweights, all_dl_dbiases = backward_pass(all_weights, all_biases, all_f, all_h, y)"
      ],
      "metadata": {
        "id": "9A9MHc4sQvbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's run a code to check your implementation of backpropagation is correct. You should have this code output successful matches between your derivatives and those of finite differences."
      ],
      "metadata": {
        "id": "axpXok-iY9NF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.set_printoptions(precision=3)\n",
        "# Make space for derivatives computed by finite differences\n",
        "all_dl_dweights_fd = [None] * (K+1)\n",
        "all_dl_dbiases_fd = [None] * (K+1)\n",
        "\n",
        "# Let's test if we have the derivatives right using finite differences\n",
        "delta_fd = 0.000001\n",
        "\n",
        "# Test the dervatives of the bias vectors\n",
        "for layer in range(K):\n",
        "  dl_dbias  = np.zeros_like(all_dl_dbiases[layer])\n",
        "  # For every element in the bias\n",
        "  for row in range(all_biases[layer].shape[0]):\n",
        "    # Take copy of biases  We'll change one element each time\n",
        "    all_biases_copy = [np.array(x) for x in all_biases]\n",
        "    all_biases_copy[layer][row] += delta_fd\n",
        "    network_output_1, *_ = compute_network_output(net_input, all_weights, all_biases_copy)\n",
        "    network_output_2, *_ = compute_network_output(net_input, all_weights, all_biases)\n",
        "    dl_dbias[row] = (least_squares_loss(network_output_1, y) - least_squares_loss(network_output_2,y))/delta_fd\n",
        "  all_dl_dbiases_fd[layer] = np.array(dl_dbias)\n",
        "  print(\"-----------------------------------------------\")\n",
        "  print(\"Bias %d, derivatives from backprop:\"%(layer))\n",
        "  print(all_dl_dbiases[layer])\n",
        "  print(\"Bias %d, derivatives from finite differences\"%(layer))\n",
        "  print(all_dl_dbiases_fd[layer])\n",
        "  if np.allclose(all_dl_dbiases_fd[layer],all_dl_dbiases[layer],rtol=1e-05, atol=1e-08, equal_nan=False):\n",
        "    print(\"Success!  Derivatives match.\")\n",
        "  else:\n",
        "    print(\"Failure!  Derivatives different.\")\n",
        "\n",
        "\n",
        "\n",
        "# Test the derivatives of the weights matrices\n",
        "for layer in range(K):\n",
        "  dl_dweight  = np.zeros_like(all_dl_dweights[layer])\n",
        "  # For every element in the bias\n",
        "  for row in range(all_weights[layer].shape[0]):\n",
        "    for col in range(all_weights[layer].shape[1]):\n",
        "      # Take copy of biases  We'll change one element each time\n",
        "      all_weights_copy = [np.array(x) for x in all_weights]\n",
        "      all_weights_copy[layer][row][col] += delta_fd\n",
        "      network_output_1, *_ = compute_network_output(net_input, all_weights_copy, all_biases)\n",
        "      network_output_2, *_ = compute_network_output(net_input, all_weights, all_biases)\n",
        "      dl_dweight[row][col] = (least_squares_loss(network_output_1, y) - least_squares_loss(network_output_2,y))/delta_fd\n",
        "  all_dl_dweights_fd[layer] = np.array(dl_dweight)\n",
        "  print(\"-----------------------------------------------\")\n",
        "  print(\"Weight %d, derivatives from backprop:\"%(layer))\n",
        "  print(all_dl_dweights[layer])\n",
        "  print(\"Weight %d, derivatives from finite differences\"%(layer))\n",
        "  print(all_dl_dweights_fd[layer])\n",
        "  if np.allclose(all_dl_dweights_fd[layer],all_dl_dweights[layer],rtol=1e-05, atol=1e-08, equal_nan=False):\n",
        "    print(\"Success!  Derivatives match.\")\n",
        "  else:\n",
        "    print(\"Failure!  Derivatives different.\")"
      ],
      "metadata": {
        "id": "PK-UtE3hreAK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c45f6cf8-dcd1-4414-fc3d-477f4722d597"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----------------------------------------------\n",
            "Bias 0, derivatives from backprop:\n",
            "[[ 0.   ]\n",
            " [46.618]\n",
            " [ 8.447]\n",
            " [ 0.   ]\n",
            " [ 0.   ]\n",
            " [ 0.   ]]\n",
            "Bias 0, derivatives from finite differences\n",
            "[[ 0.   ]\n",
            " [46.618]\n",
            " [ 8.447]\n",
            " [ 0.   ]\n",
            " [ 0.   ]\n",
            " [ 0.   ]]\n",
            "Success!  Derivatives match.\n",
            "-----------------------------------------------\n",
            "Bias 1, derivatives from backprop:\n",
            "[[ 0.   ]\n",
            " [ 0.   ]\n",
            " [12.372]\n",
            " [ 0.   ]\n",
            " [70.185]\n",
            " [10.208]]\n",
            "Bias 1, derivatives from finite differences\n",
            "[[ 0.   ]\n",
            " [ 0.   ]\n",
            " [12.372]\n",
            " [ 0.   ]\n",
            " [70.185]\n",
            " [10.208]]\n",
            "Success!  Derivatives match.\n",
            "-----------------------------------------------\n",
            "Bias 2, derivatives from backprop:\n",
            "[[-34.754]\n",
            " [  0.   ]\n",
            " [  0.   ]\n",
            " [  0.   ]\n",
            " [  0.   ]\n",
            " [ 24.769]]\n",
            "Bias 2, derivatives from finite differences\n",
            "[[-34.754]\n",
            " [  0.   ]\n",
            " [  0.   ]\n",
            " [  0.   ]\n",
            " [  0.   ]\n",
            " [ 24.769]]\n",
            "Success!  Derivatives match.\n",
            "-----------------------------------------------\n",
            "Bias 3, derivatives from backprop:\n",
            "[[  0.   ]\n",
            " [-24.858]\n",
            " [ 27.016]\n",
            " [  0.   ]\n",
            " [ 10.873]\n",
            " [  0.   ]]\n",
            "Bias 3, derivatives from finite differences\n",
            "[[  0.   ]\n",
            " [-24.858]\n",
            " [ 27.016]\n",
            " [  0.   ]\n",
            " [ 10.873]\n",
            " [  0.   ]]\n",
            "Success!  Derivatives match.\n",
            "-----------------------------------------------\n",
            "Bias 4, derivatives from backprop:\n",
            "[[ 0.   ]\n",
            " [-2.708]\n",
            " [ 0.   ]\n",
            " [ 0.   ]\n",
            " [29.791]\n",
            " [ 5.576]]\n",
            "Bias 4, derivatives from finite differences\n",
            "[[ 0.   ]\n",
            " [-2.708]\n",
            " [ 0.   ]\n",
            " [ 0.   ]\n",
            " [29.791]\n",
            " [ 5.576]]\n",
            "Success!  Derivatives match.\n",
            "-----------------------------------------------\n",
            "Weight 0, derivatives from backprop:\n",
            "[[ 0.   ]\n",
            " [55.941]\n",
            " [10.136]\n",
            " [ 0.   ]\n",
            " [ 0.   ]\n",
            " [ 0.   ]]\n",
            "Weight 0, derivatives from finite differences\n",
            "[[ 0.   ]\n",
            " [55.941]\n",
            " [10.136]\n",
            " [ 0.   ]\n",
            " [ 0.   ]\n",
            " [ 0.   ]]\n",
            "Success!  Derivatives match.\n",
            "-----------------------------------------------\n",
            "Weight 1, derivatives from backprop:\n",
            "[[  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.     27.034   6.727   0.      0.      0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.    153.359  38.161   0.      0.      0.   ]\n",
            " [  0.     22.305   5.55    0.      0.      0.   ]]\n",
            "Weight 1, derivatives from finite differences\n",
            "[[  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.     27.034   6.727   0.      0.      0.   ]\n",
            " [  0.      0.      0.      0.      0.      0.   ]\n",
            " [  0.    153.359  38.161   0.      0.      0.   ]\n",
            " [  0.     22.305   5.55    0.      0.      0.   ]]\n",
            "Success!  Derivatives match.\n",
            "-----------------------------------------------\n",
            "Weight 2, derivatives from backprop:\n",
            "[[   0.       0.     -54.442    0.      -5.143 -163.122]\n",
            " [   0.       0.       0.       0.       0.       0.   ]\n",
            " [   0.       0.       0.       0.       0.       0.   ]\n",
            " [   0.       0.       0.       0.       0.       0.   ]\n",
            " [   0.       0.       0.       0.       0.       0.   ]\n",
            " [   0.       0.      38.8      0.       3.666  116.255]]\n",
            "Weight 2, derivatives from finite differences\n",
            "[[   0.       0.     -54.442    0.      -5.143 -163.122]\n",
            " [   0.       0.       0.       0.       0.       0.   ]\n",
            " [   0.       0.       0.       0.       0.       0.   ]\n",
            " [   0.       0.       0.       0.       0.       0.   ]\n",
            " [   0.       0.       0.       0.       0.       0.   ]\n",
            " [   0.       0.      38.8      0.       3.666  116.255]]\n",
            "Success!  Derivatives match.\n",
            "-----------------------------------------------\n",
            "Weight 3, derivatives from backprop:\n",
            "[[   0.       0.       0.       0.       0.       0.   ]\n",
            " [ -26.622    0.       0.       0.       0.    -101.347]\n",
            " [  28.933    0.       0.       0.       0.     110.145]\n",
            " [   0.       0.       0.       0.       0.       0.   ]\n",
            " [  11.644    0.       0.       0.       0.      44.328]\n",
            " [   0.       0.       0.       0.       0.       0.   ]]\n",
            "Weight 3, derivatives from finite differences\n",
            "[[   0.       0.       0.       0.       0.       0.   ]\n",
            " [ -26.622    0.       0.       0.       0.    -101.347]\n",
            " [  28.933    0.       0.       0.       0.     110.145]\n",
            " [   0.       0.       0.       0.       0.       0.   ]\n",
            " [  11.644    0.       0.       0.       0.      44.328]\n",
            " [   0.       0.       0.       0.       0.       0.   ]]\n",
            "Success!  Derivatives match.\n",
            "-----------------------------------------------\n",
            "Weight 4, derivatives from backprop:\n",
            "[[ 0.     0.     0.     0.     0.     0.   ]\n",
            " [ 0.    -1.976 -8.5    0.    -0.416  0.   ]\n",
            " [ 0.     0.     0.     0.     0.     0.   ]\n",
            " [ 0.     0.     0.     0.     0.     0.   ]\n",
            " [ 0.    21.73  93.499  0.     4.571  0.   ]\n",
            " [ 0.     4.067 17.5    0.     0.856  0.   ]]\n",
            "Weight 4, derivatives from finite differences\n",
            "[[ 0.     0.     0.     0.     0.     0.   ]\n",
            " [ 0.    -1.976 -8.5    0.    -0.416  0.   ]\n",
            " [ 0.     0.     0.     0.     0.     0.   ]\n",
            " [ 0.     0.     0.     0.     0.     0.   ]\n",
            " [ 0.    21.73  93.499  0.     4.571  0.   ]\n",
            " [ 0.     4.067 17.5    0.     0.856  0.   ]]\n",
            "Success!  Derivatives match.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-5dd321eac003>:44: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  dl_dweight[row][col] = (least_squares_loss(network_output_1, y) - least_squares_loss(network_output_2,y))/delta_fd\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K_2tLzutNUf4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}